{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Note: you may have to add/clone/checkout some of these packages\n",
    "Notebook based on Tom Berloff works: http://www.breloff.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mINFO: Recompiling stale cache file /Users/ppalmes/.julia/lib/v0.5/Learn.ji for module Learn.\n",
      "\u001b[0m\u001b[1m\u001b[31mERROR: LoadError: syntax: expected identifier after \"const\"\n",
      " in include_from_node1(::String) at ./loading.jl:488\n",
      " in include_from_node1(::String) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in macro expansion; at ./none:2 [inlined]\n",
      " in anonymous at ./<missing>:?\n",
      " in eval(::Module, ::Any) at ./boot.jl:234\n",
      " in eval(::Module, ::Any) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in process_options(::Base.JLOptions) at ./client.jl:242\n",
      " in _start() at ./client.jl:321\n",
      " in _start() at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      "while loading /Users/ppalmes/.julia/v0.5/PenaltyFunctions/src/PenaltyFunctions.jl, in expression starting on line 28\n",
      "\u001b[0m\u001b[1m\u001b[31mERROR: LoadError: Failed to precompile PenaltyFunctions to /Users/ppalmes/.julia/lib/v0.5/PenaltyFunctions.ji.\n",
      " in compilecache(::String) at ./loading.jl:593\n",
      " in require(::Symbol) at ./loading.jl:393\n",
      " in require(::Symbol) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in include_from_node1(::String) at ./loading.jl:488\n",
      " in include_from_node1(::String) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in macro expansion; at ./none:2 [inlined]\n",
      " in anonymous at ./<missing>:?\n",
      " in eval(::Module, ::Any) at ./boot.jl:234\n",
      " in eval(::Module, ::Any) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in process_options(::Base.JLOptions) at ./client.jl:242\n",
      " in _start() at ./client.jl:321\n",
      " in _start() at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      "while loading /Users/ppalmes/.julia/v0.5/ObjectiveFunctions/src/ObjectiveFunctions.jl, in expression starting on line 9\n",
      "\u001b[0m\u001b[1m\u001b[31mERROR: LoadError: Failed to precompile ObjectiveFunctions to /Users/ppalmes/.julia/lib/v0.5/ObjectiveFunctions.ji.\n",
      " in compilecache(::String) at ./loading.jl:593\n",
      " in require(::Symbol) at ./loading.jl:393\n",
      " in require(::Symbol) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in include_from_node1(::String) at ./loading.jl:488\n",
      " in include_from_node1(::String) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in macro expansion; at ./none:2 [inlined]\n",
      " in anonymous at ./<missing>:?\n",
      " in eval(::Module, ::Any) at ./boot.jl:234\n",
      " in eval(::Module, ::Any) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      " in process_options(::Base.JLOptions) at ./client.jl:242\n",
      " in _start() at ./client.jl:321\n",
      " in _start() at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?\n",
      "while loading /Users/ppalmes/.julia/v0.5/Learn/src/Learn.jl, in expression starting on line 9\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Failed to precompile Learn to /Users/ppalmes/.julia/lib/v0.5/Learn.ji.",
     "output_type": "error",
     "traceback": [
      "Failed to precompile Learn to /Users/ppalmes/.julia/lib/v0.5/Learn.ji.",
      "",
      " in compilecache(::String) at ./loading.jl:593",
      " in require(::Symbol) at ./loading.jl:393",
      " in require(::Symbol) at /Applications/Julia-0.5.app/Contents/Resources/julia/lib/julia/sys.dylib:?"
     ]
    }
   ],
   "source": [
    "# this re-exports Transformations, StochasticOptimization, Penalties, and ObjectiveFunctions\n",
    "using Learn\n",
    "using MLPlots\n",
    "\n",
    "# my version of ML iteration.  Hopefully will be replaced with what's currently in MLDataUtils dev branch\n",
    "using StochasticOptimization.Iteration\n",
    "\n",
    "import MLDataUtils: rescale!\n",
    "\n",
    "# for loading the data\n",
    "import MNIST\n",
    "\n",
    "# for plotting\n",
    "using StatPlots, MLPlots\n",
    "#gr(leg=false, linealpha=0.5, legendfont=font(7), fmt=:png)\n",
    "gr(leg=false, linealpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_test_loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one-hot matrix given class labels\n",
    "# TODO: this should be added as a utility in MLDataUtils\n",
    "function to_one_hot(y::AbstractVector)\n",
    "    yint = map(yi->round(Int,yi)+1, y)\n",
    "    nclasses = maximum(yint)\n",
    "    hot = zeros(Float64, nclasses, length(y))\n",
    "    for (i,yi) in enumerate(yint)\n",
    "        hot[yi,i] = 1.0\n",
    "    end\n",
    "    hot\n",
    "end\n",
    "\n",
    "# randomly pick a subset of testdata (size = totcount) and compute the total loss\n",
    "function my_test_loss(obj, testdata, totcount = 500)\n",
    "    totloss = 0.0\n",
    "    totcorrect = 0\n",
    "    for (x,y) in each_obs(rand(each_obs(testdata), totcount))\n",
    "        totloss += transform!(obj,y,x)\n",
    "\n",
    "        # logistic version:\n",
    "        # ŷ = output_value(obj.transformation)[1]\n",
    "        # correct = (ŷ > 0.5 && y > 0.5) || (ŷ <= 0.5 && y < 0.5)\n",
    "\n",
    "        # softmax version:\n",
    "        ŷ = output_value(obj.transformation)\n",
    "        chosen_idx = indmax(ŷ)\n",
    "        correct = y[chosen_idx] > 0\n",
    "\n",
    "        totcorrect += correct\n",
    "    end\n",
    "    totloss, totcorrect/totcount\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: MNIST not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: MNIST not defined",
      ""
     ]
    }
   ],
   "source": [
    "# our data:\n",
    "x_train, y_train = MNIST.traindata()\n",
    "x_test, y_test = MNIST.testdata()\n",
    "\n",
    "# normalize the input data given μ/σ for the input training data\n",
    "# note: scale both train and test sets using the train data\n",
    "μ, σ = rescale!(x_train)\n",
    "rescale!(x_test, μ, σ)\n",
    "# xmin, xmax = extrema(x_train)\n",
    "# x_train .= 2 .* (x_train .- xmin) ./ (xmax - xmin) .- 1\n",
    "# x_test .= 2 .* (x_test .- xmin) ./ (xmax - xmin) .- 1\n",
    "\n",
    "# convert y data to one-hot\n",
    "y_train, y_test = map(to_one_hot, (y_train, y_test))\n",
    "\n",
    "# optional: limit to only 0/1 digits for easier training\n",
    "# to_isone(y::AbstractVector) = (z = Array(eltype(y), 1, length(y)); map!(yi->float(yi==1.0), z, y))\n",
    "# y_train, y_test = map(to_isone, (y_train, y_test))\n",
    "# train = filterobs(i -> y_train[i] < 1.5, x_train, y_train)\n",
    "# test = filterobs(i -> y_test[i] < 1.5, x_test, y_test)\n",
    "\n",
    "# store as tuples to make it easier\n",
    "train = (x_train, y_train)\n",
    "test = (x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct our model and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: nnet not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: nnet not defined",
      ""
     ]
    }
   ],
   "source": [
    "# Activation Functions => :logistic, :tanh, :softsign, :relu, :softplus, :sinusoid, :gaussian, :threshold, :sign\n",
    "\n",
    "nin, nh, nout = 784, [50,50], 10\n",
    "\n",
    "# this is our gradient calculation method\n",
    "\n",
    "grad_calc = :backprop\n",
    "#grad_calc = :dfa\n",
    "\n",
    "# create a feedforward neural net with softplus activations and softmax output\n",
    "t = nnet(nin, nout, nh, :sign, :softmax, grad_calc=grad_calc)\n",
    "\n",
    "# create an objective function with L2 penalty and an implicit cross entropy loss layer\n",
    "penalty = NoPenalty()\n",
    "obj = objective(t, penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional: set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: ChainPlot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ChainPlot not defined",
      ""
     ]
    }
   ],
   "source": [
    "# the parts of the plot\n",
    "chainplt = ChainPlot(t, maxn=10, tickfont=font(5))\n",
    "lossplt = TracePlot(2, title=\"Loss\", ylim=(0,Inf), leg=true, lab=[\"Train\" \"Test\"])\n",
    "accuracyplt = TracePlot(2, title=\"Accuracy\", ylim=(0.4,1), leg=true, lab=[\"Train\" \"Test\"])\n",
    "hmplt = heatmap(rand(28,28), ratio=1, title=\"outgoing wgt\")\n",
    "hmplt2 = heatmap(rand(28,28), ratio=1, title=\"input grad\")\n",
    "\n",
    "# put together the full plot... a ChainPlot with loss, accuracy, and the heatmap\n",
    "plot(\n",
    "    chainplt.plt,\n",
    "    lossplt.plt,\n",
    "    accuracyplt.plt,\n",
    "    hmplt,\n",
    "    hmplt2,\n",
    "    size = (1200,800),\n",
    "    layout=@layout([a{0.8h}; grid(1,2){0.75w} grid(1,2)])\n",
    ")\n",
    "\n",
    "anim = nothing\n",
    "#anim = Animation()\n",
    "\n",
    "# this is our custom callback which will be called on every 100 iterations\n",
    "# note: we do the plotting here.\n",
    "tracer = IterFunction((obj, i) -> begin\n",
    "    # sample points from the test set and compute/save the loss\n",
    "    @show i\n",
    "    if mod1(i,500)==500\n",
    "        # training loss\n",
    "        trainloss, trainaccuracy = my_test_loss(obj, train, 200)\n",
    "        @show trainloss, trainaccuracy\n",
    "        \n",
    "        testloss, testaccuracy = my_test_loss(obj, test, 200)\n",
    "        @show testloss, testaccuracy\n",
    "        \n",
    "        push!(lossplt, i, [trainloss, testloss])\n",
    "        push!(accuracyplt, i, [trainaccuracy, testaccuracy])\n",
    "    end\n",
    "\n",
    "    # add transformation data\n",
    "    update!(chainplt)\n",
    "\n",
    "    # update the heatmap of the total outgoing weight from each pixel\n",
    "    t1 = isa(t[1], InputNorm) ? t[2] : t[1]\n",
    "    pixel_importance = reshape(sum(t1.params.views[1],1), 28, 28)\n",
    "    hmplt[1][1][:z].surf[:] = pixel_importance\n",
    "    \n",
    "    pixel_importance = reshape(abs(input_grad(t)),28,28)  # another possible metric\n",
    "    hmplt2[1][1][:z].surf[:] = pixel_importance\n",
    "\n",
    "    # handle animation frames/output\n",
    "    anim == nothing || frame(anim)\n",
    "\n",
    "    # update the plot display\n",
    "    #gui()\n",
    "    inline()\n",
    "end, every=100)\n",
    "\n",
    "# trace once before we start learning to see initial values\n",
    "tracer.f(obj, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MetaLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: make_learner not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: make_learner not defined",
      ""
     ]
    }
   ],
   "source": [
    "learner = make_learner(\n",
    "    # averages the gradient over minibatches, updating params using the Adam method\n",
    "    GradientLearner(1e-3, SGD(0.7)),\n",
    "\n",
    "    # our custom iteration method\n",
    "    tracer,\n",
    "\n",
    "    # shorthand to add a MaxIter(10000)\n",
    "    maxiter = 1500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: learn! not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: learn! not defined",
      ""
     ]
    }
   ],
   "source": [
    "# do the learning... average over minibatches of size 5 for maxiter iterations\n",
    "# learn!(obj, learner, infinite_batches(train, size=5))\n",
    "learn!(obj, learner, infinite_obs(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dfa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ":dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
